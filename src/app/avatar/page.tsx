'use client'

import { useEffect, useRef, useState } from 'react'
import { useLanguage, type Language } from '@/contexts/language-context'

// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–æ–ª–æ—Å–æ–≤ Azure –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞
const VOICE_CONFIG: Record<Language, { voice: string; xmlLang: string; speechRecognitionLang: string }> = {
  ru: {
    voice: 'ru-RU-SvetlanaNeural',
    xmlLang: 'ru-RU',
    speechRecognitionLang: 'ru-RU'
  },
  kk: {
    voice: 'kk-KZ-AigulNeural',
    xmlLang: 'kk-KZ',
    speechRecognitionLang: 'kk-KZ'
  },
  en: {
    voice: 'en-US-AriaNeural',
    xmlLang: 'en-US',
    speechRecognitionLang: 'en-US'
  }
}

// –Ø–∑—ã–∫–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
const LANG_INSTRUCTIONS: Record<Language, string> = {
  ru: "[–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –û—Ç–≤–µ—Ç—å –Ω–∞ –†–£–°–°–ö–û–ú —è–∑—ã–∫–µ. –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∫ —Ñ—É–Ω–∫—Ü–∏—è–º —Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º.]",
  kk: "[–ù“∞–°“ö–ê–£–õ–´“ö: “ö–ê–ó–ê“ö —Ç—ñ–ª—ñ–Ω–¥–µ –∂–∞—É–∞–ø –±–µ—Ä—ñ“£—ñ–∑. –ë–∞—Ä–ª—ã“õ —Ñ—É–Ω–∫—Ü–∏—è —Å“±—Ä–∞—É–ª–∞—Ä—ã–Ω –æ—Ä—ã—Å —Ç—ñ–ª—ñ–Ω–¥–µ –∂–∞—Å–∞“£—ã–∑.]",
  en: "[INSTRUCTION: Respond in ENGLISH language. Formulate all function queries in Russian language.]"
}

// –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö
const GREETINGS: Record<Language, string> = {
  ru: "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?",
  kk: "–°”ô–ª–µ–º–µ—Ç—Å—ñ–∑ –±–µ! –ú–µ–Ω –≤–∏—Ä—Ç—É–∞–ª–¥—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–ø—ñ–Ω. “ö–∞–ª–∞–π –∫”©–º–µ–∫—Ç–µ—Å–µ –∞–ª–∞–º—ã–Ω?",
  en: "Hello! I am a virtual assistant. How can I help you?"
}

// –õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è UI
const UI_TEXT: Record<Language, {
  title: string
  videoTitle: string
  connected: string
  connecting: string
  notConnected: string
  loadingSdk: string
  notConnectedLabel: string
  startButton: string
  stopButton: string
  dialogTitle: string
  speaking: string
  listening: string
  microphoneButton: string
  sendButton: string
  inputPlaceholder: string
  avatarConnected: string
  speechRecognitionFailed: string
  requestError: string
  noAnswer: string
}> = {
  ru: {
    title: "ü§ñ AI –ê–≤–∞—Ç–∞—Ä - JIbek",
    videoTitle: "–í–∏–¥–µ–æ –∞–≤–∞—Ç–∞—Ä",
    connected: "üü¢ –ü–æ–¥–∫–ª—é—á–µ–Ω",
    connecting: "üü° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ...",
    notConnected: "‚ö™ –ù–µ –ø–æ–¥–∫–ª—é—á–µ–Ω",
    loadingSdk: "–ó–∞–≥—Ä—É–∑–∫–∞ SDK...",
    notConnectedLabel: "–ê–≤–∞—Ç–∞—Ä –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω",
    startButton: "üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤–∞—Ç–∞—Ä",
    stopButton: "‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å",
    dialogTitle: "–î–∏–∞–ª–æ–≥",
    speaking: "üéôÔ∏è –ì–æ–≤–æ—Ä—é...",
    listening: "–°–ª—É—à–∞—é...",
    microphoneButton: "üéôÔ∏è –ú–∏–∫—Ä–æ—Ñ–æ–Ω",
    sendButton: "–û—Ç–ø—Ä–∞–≤–∏—Ç—å",
    inputPlaceholder: "–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å...",
    avatarConnected: "–ê–≤–∞—Ç–∞—Ä –ø–æ–¥–∫–ª—é—á–µ–Ω! –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å.",
    speechRecognitionFailed: "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.",
    requestError: "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞",
    noAnswer: "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç"
  },
  kk: {
    title: "ü§ñ AI –ê–≤–∞—Ç–∞—Ä - JIbek",
    videoTitle: "–ë–µ–π–Ω–µ –∞–≤–∞—Ç–∞—Ä",
    connected: "üü¢ “ö–æ—Å—ã–ª–¥—ã",
    connecting: "üü° “ö–æ—Å—ã–ª—É–¥–∞...",
    notConnected: "‚ö™ “ö–æ—Å—ã–ª–º–∞“ì–∞–Ω",
    loadingSdk: "SDK –∂“Ø–∫—Ç–µ–ª—É–¥–µ...",
    notConnectedLabel: "–ê–≤–∞—Ç–∞—Ä “õ–æ—Å—ã–ª–º–∞“ì–∞–Ω",
    startButton: "üöÄ –ê–≤–∞—Ç–∞—Ä–¥—ã —ñ—Å–∫–µ “õ–æ—Å—É",
    stopButton: "‚èπÔ∏è –¢–æ“õ—Ç–∞—Ç—É",
    dialogTitle: "–î–∏–∞–ª–æ–≥",
    speaking: "üéôÔ∏è –ê–π—Ç—ã–ø –∂–∞—Ç—ã—Ä–º—ã–Ω...",
    listening: "–¢—ã“£–¥–∞–ø –∂–∞—Ç—ã—Ä–º—ã–Ω...",
    microphoneButton: "üéôÔ∏è –ú–∏–∫—Ä–æ—Ñ–æ–Ω",
    sendButton: "–ñ—ñ–±–µ—Ä—É",
    inputPlaceholder: "–°“±—Ä–∞“ì—ã“£—ã–∑–¥—ã –µ–Ω–≥—ñ–∑—ñ“£—ñ–∑...",
    avatarConnected: "–ê–≤–∞—Ç–∞—Ä “õ–æ—Å—ã–ª–¥—ã! –°“±—Ä–∞“õ “õ–æ–π—ã“£—ã–∑.",
    speechRecognitionFailed: "–°”©–∑–¥—ñ —Ç–∞–Ω—É –º“Ø–º–∫—ñ–Ω –±–æ–ª–º–∞–¥—ã, “õ–∞–π—Ç–∞–ª–∞–ø –∫”©—Ä—ñ“£—ñ–∑.",
    requestError: "–°“±—Ä–∞—É–¥—ã ”©“£–¥–µ—É –∫–µ–∑—ñ–Ω–¥–µ “õ–∞—Ç–µ",
    noAnswer: "–ñ–∞—É–∞–ø –∞–ª—É –º“Ø–º–∫—ñ–Ω –±–æ–ª–º–∞–¥—ã"
  },
  en: {
    title: "ü§ñ AI Avatar - JIbek",
    videoTitle: "Video avatar",
    connected: "üü¢ Connected",
    connecting: "üü° Connecting...",
    notConnected: "‚ö™ Not connected",
    loadingSdk: "Loading SDK...",
    notConnectedLabel: "Avatar not connected",
    startButton: "üöÄ Start avatar",
    stopButton: "‚èπÔ∏è Stop",
    dialogTitle: "Dialog",
    speaking: "üéôÔ∏è Speaking...",
    listening: "Listening...",
    microphoneButton: "üéôÔ∏è Microphone",
    sendButton: "Send",
    inputPlaceholder: "Enter your question...",
    avatarConnected: "Avatar connected! Ask a question.",
    speechRecognitionFailed: "Failed to recognize speech, please try again.",
    requestError: "Error processing request",
    noAnswer: "Failed to get answer"
  }
}

export default function AvatarPage() {
  // –Ø–∑—ã–∫–æ–≤–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —è–∑—ã–∫ –∏–∑ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
  const { language } = useLanguage()

  const [isInitialized, setIsInitialized] = useState(false)
  const [isConnecting, setIsConnecting] = useState(false)
  const [isConnected, setIsConnected] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [userInput, setUserInput] = useState('')
  const [messages, setMessages] = useState<Array<{role: string, text: string}>>([])
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [isListening, setIsListening] = useState(false)
  const [listeningText, setListeningText] = useState('')
  
  const AZURE_REGION = 'eastus2'
  const AZURE_SPEECH_KEY = '[REMOVED]'

  // –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —è–∑—ã–∫–∞
  const t = UI_TEXT[language]

  const videoRef = useRef<HTMLVideoElement>(null)
  const audioRef = useRef<HTMLAudioElement>(null)
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null)
  const avatarSynthesizerRef = useRef<any>(null)

  // –ó–∞–≥—Ä—É–∑–∫–∞ Azure Speech SDK
  useEffect(() => {
    const script = document.createElement('script')
    script.src = 'https://aka.ms/csspeech/jsbrowserpackageraw'
    script.async = true
    script.onload = () => {
      console.log('‚úÖ Azure Speech SDK loaded')
      setIsInitialized(true)
    }
    script.onerror = () => {
      setError('Failed to load Azure Speech SDK')
    }
    document.body.appendChild(script)

    return () => {
      if (peerConnectionRef.current) {
        peerConnectionRef.current.close()
      }
      if (avatarSynthesizerRef.current) {
        avatarSynthesizerRef.current.close()
      }
    }
  }, [])

  const connectAvatar = async () => {
    if (!isInitialized) {
      setError('SDK not initialized')
      return
    }

    setIsConnecting(true)
    setError(null)

    try {
      const SpeechSDK = (window as any).SpeechSDK
      
      // –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Azure Speech
      const cogSvcRegion = AZURE_REGION
      const cogSvcSubKey = AZURE_SPEECH_KEY
      
      const speechSynthesisConfig = SpeechSDK.SpeechConfig.fromSubscription(cogSvcSubKey, cogSvcRegion)
      
      // –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≤–∞—Ç–∞—Ä–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –∞–≤–∞—Ç–∞—Ä "lisa"
      const avatarConfig = new SpeechSDK.AvatarConfig('lisa', 'casual-sitting')
      avatarConfig.customized = false
      
      avatarSynthesizerRef.current = new SpeechSDK.AvatarSynthesizer(speechSynthesisConfig, avatarConfig)
      
      console.log('üé§ Avatar synthesizer created')
      
      // –ü–æ–ª—É—á–∞–µ–º WebRTC —Ç–æ–∫–µ–Ω
      const response = await fetch(
        `https://${cogSvcRegion}.tts.speech.microsoft.com/cognitiveservices/avatar/relay/token/v1`,
        {
          headers: {
            'Ocp-Apim-Subscription-Key': cogSvcSubKey
          }
        }
      )
      
      if (!response.ok) {
        throw new Error(`Failed to get token: ${response.status}`)
      }
      
      const tokenData = await response.json()
      
      // –ù–∞—Å—Ç—Ä–æ–π–∫–∞ WebRTC
      const peerConnection = new RTCPeerConnection({
        iceServers: [{
          urls: tokenData.Urls,
          username: tokenData.Username,
          credential: tokenData.Password
        }]
      })

      peerConnectionRef.current = peerConnection

      // –Ø–≤–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ –∫–∞–Ω–∞–ª—ã —É Azure, –∏–Ω–∞—á–µ –ø–æ—Ç–æ–∫ –Ω–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç
      peerConnection.addTransceiver('video', { direction: 'sendrecv' })
      peerConnection.addTransceiver('audio', { direction: 'sendrecv' })

      // –°–æ–∑–¥–∞—ë–º —Å–ª—É–∂–µ–±–Ω—ã–π data channel –∏ —Å–ª—É—à–∞–µ–º —Å–æ–±—ã—Ç–∏—è —Å–µ—Ä–≤–µ—Ä–∞ (—Å—É–±—Ç–∏—Ç—Ä—ã, idle –∏ —Ç.–ø.)
      const clientEventChannel = peerConnection.createDataChannel('clientEvent')
      clientEventChannel.onmessage = (event) => {
        console.log('üì® Avatar client event:', event.data)
      }

      peerConnection.addEventListener('datachannel', (event) => {
        const channel = event.channel
        channel.onmessage = (e) => {
          console.log('üì° Avatar server event:', e.data)
        }
      })

      // –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ –ø–æ—Ç–æ–∫–æ–≤
      peerConnection.ontrack = (event) => {
        console.log('üì° Track received:', event.track.kind)

        if (event.track.kind === 'video' && videoRef.current) {
          videoRef.current.srcObject = event.streams[0]
          videoRef.current.autoplay = true
          videoRef.current.playsInline = true
          console.log('üé• Video connected')
        }
        
        if (event.track.kind === 'audio' && audioRef.current) {
          audioRef.current.srcObject = event.streams[0]
          audioRef.current.autoplay = true
          console.log('üîä Audio connected')
        }
      }

      peerConnection.oniceconnectionstatechange = () => {
        console.log('ICE connection state:', peerConnection.iceConnectionState)
        const state = peerConnection.iceConnectionState

        if (state === 'connected' || state === 'completed') {
          setIsConnected(true)
          setIsConnecting(false)
          setMessages((prev) => prev.length > 0 ? prev : [{ role: 'system', text: t.avatarConnected }])
        } else if (state === 'failed') {
          setIsConnecting(false)
          setIsConnected(false)
          setError('WebRTC connection failed. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª—é—á –∏ —Ä–µ–≥–∏–æ–Ω Azure Speech.')
        } else if (state === 'disconnected' || state === 'closed') {
          setIsConnecting(false)
          setIsConnected(false)
        }
      }

      // –ó–∞–ø—É—Å–∫–∞–µ–º –∞–≤–∞—Ç–∞—Ä
      const startResult = await avatarSynthesizerRef.current.startAvatarAsync(peerConnection)
      if (startResult.reason !== SpeechSDK.ResultReason.SynthesizingAudioCompleted) {
        if (startResult.reason === SpeechSDK.ResultReason.Canceled) {
          const cancellation = SpeechSDK.CancellationDetails.fromResult(startResult)
          throw new Error(cancellation.errorDetails || 'Avatar start canceled')
        }
        throw new Error('Avatar failed to start')
      }
      console.log('‚úÖ Avatar started', startResult.resultId)
      
      // –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —è–∑—ã–∫–µ
      setIsSpeaking(true)
      const voiceConfig = VOICE_CONFIG[language]
      const greeting = GREETINGS[language]
      
      const ssml = `<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xmlns:mstts="http://www.w3.org/2001/mstts" xml:lang="${voiceConfig.xmlLang}">
        <voice name="${voiceConfig.voice}">
          <mstts:leadingsilence-exact value="0"/>
          <prosody rate="0%" pitch="0%">
            ${greeting}
          </prosody>
        </voice>
      </speak>`
      
      console.log(`üëã Greeting with voice: ${voiceConfig.voice}, Language: ${language}`)
      
      const greetingResult = await avatarSynthesizerRef.current.speakSsmlAsync(ssml)
      if (greetingResult.reason !== SpeechSDK.ResultReason.SynthesizingAudioCompleted) {
        if (greetingResult.reason === SpeechSDK.ResultReason.Canceled) {
          const cancellation = SpeechSDK.CancellationDetails.fromResult(greetingResult)
          console.error('Greeting canceled:', cancellation.errorDetails)
        } else {
          console.error('Greeting failed with reason:', greetingResult.reason)
        }
      }
      setIsSpeaking(false)
      
    } catch (err: any) {
      console.error('‚ùå Connection error:', err)
      setError(err.message)
      setIsConnecting(false)
      setIsSpeaking(false)
    }
  }

  const speak = async (text: string) => {
    if (!avatarSynthesizerRef.current || !isConnected) {
      console.error('Avatar not connected')
      return
    }
    
    setIsSpeaking(true)
    
    const SpeechSDK = (window as any).SpeechSDK
    const voiceConfig = VOICE_CONFIG[language]
    
    const ssml = `<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xmlns:mstts="http://www.w3.org/2001/mstts" xml:lang="${voiceConfig.xmlLang}">
      <voice name="${voiceConfig.voice}">
        <mstts:leadingsilence-exact value="0"/>
        <prosody rate="0%" pitch="0%">
          ${text}
        </prosody>
      </voice>
    </speak>`
    
    console.log(`üîä Speaking with voice: ${voiceConfig.voice}, Language: ${language}`)
    
    try {
      const speechResult = await avatarSynthesizerRef.current.speakSsmlAsync(ssml)
      if (speechResult.reason !== SpeechSDK.ResultReason.SynthesizingAudioCompleted) {
        if (speechResult.reason === SpeechSDK.ResultReason.Canceled) {
          const cancellation = SpeechSDK.CancellationDetails.fromResult(speechResult)
          console.error('Speech canceled:', cancellation.errorDetails)
        } else {
          console.error('Speech failed with reason:', speechResult.reason)
        }
      }
      setIsSpeaking(false)
    } catch (err) {
      console.error('Speech error:', err)
      setIsSpeaking(false)
    }
  }

  const sendQuestion = async (question: string) => {
    const trimmedQuestion = question.trim()
    if (!trimmedQuestion) return

    const userMessage = { role: 'user', text: trimmedQuestion }
    setMessages(prev => [...prev, userMessage])

    try {
      // –î–æ–±–∞–≤–ª—è–µ–º —è–∑—ã–∫–æ–≤—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∫ –∑–∞–ø—Ä–æ—Å—É
      const languageInstruction = LANG_INSTRUCTIONS[language]
      const messageWithLanguage = `${languageInstruction}\n\n${trimmedQuestion}`
      
      console.log('üåê –ó–∞–ø—Ä–æ—Å –∫ /api/assistant')
      console.log('üì¶ Payload:', { function_name: 'db_query', arguments: { message: messageWithLanguage } })
      console.log('üåç –Ø–∑—ã–∫ –∞–≤–∞—Ç–∞—Ä–∞:', language)

      const response = await fetch('/api/assistant', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          function_name: 'db_query',
          arguments: { message: messageWithLanguage }
        })
      })

      console.log('üì• Response status:', response.status)

      const data = await response.json()
      console.log('üìä Response data:', data)

      const answer = data.result || t.noAnswer

      const assistantMessage = { role: 'assistant', text: answer }
      setMessages(prev => [...prev, assistantMessage])

      // –û–∑–≤—É—á–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç
      if (isConnected) {
        await speak(answer)
      }
    } catch (err) {
      console.error('Error:', err)
      const errorMsg = { role: 'system', text: t.requestError }
      setMessages(prev => [...prev, errorMsg])
    }
  }

  const startSpeechRecognition = async () => {
    if (!isInitialized) {
      setError('SDK not initialized')
      return
    }

    const SpeechSDK = (window as any).SpeechSDK
    if (!SpeechSDK) {
      setError('Azure Speech SDK –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω')
      return
    }

    try {
      setIsListening(true)
      setListeningText(t.listening)

      const voiceConfig = VOICE_CONFIG[language]
      const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(AZURE_SPEECH_KEY, AZURE_REGION)
      speechConfig.speechRecognitionLanguage = voiceConfig.speechRecognitionLang
      
      console.log('üé§ Starting speech recognition with language:', voiceConfig.speechRecognitionLang)
      
      const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput()
      const recognizer = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig)

      recognizer.recognizeOnceAsync(
        async (result: any) => {
          recognizer.close()
          setIsListening(false)
          setListeningText('')

          if (result.reason === SpeechSDK.ResultReason.RecognizedSpeech) {
            const recognizedText = result.text.trim()
            console.log('üìù Recognized text:', recognizedText)
            if (recognizedText) {
              await sendQuestion(recognizedText)
            } else {
              setMessages(prev => [...prev, { role: 'system', text: t.speechRecognitionFailed }])
            }
          } else if (result.reason === SpeechSDK.ResultReason.NoMatch) {
            console.warn('Speech recognition: No match')
            setMessages(prev => [...prev, { role: 'system', text: t.speechRecognitionFailed }])
          }
        },
        (err: any) => {
          recognizer.close()
          console.error('Speech recognition error:', err)
          setIsListening(false)
          setListeningText('')
          setMessages(prev => [...prev, { role: 'system', text: t.speechRecognitionFailed }])
        }
      )
    } catch (err: any) {
      console.error('Speech recognition failed:', err)
      setIsListening(false)
      setListeningText('')
      setError(err.message || 'Speech recognition failed')
    }
  }

  const handleSendMessage = async () => {
    if (!userInput.trim()) return
    
    console.log('üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è:', userInput)
    const question = userInput
    setUserInput('')
    await sendQuestion(question)
  }

  return (
    <div className="min-h-screen bg-gradient-to-br from-purple-50 to-blue-50 p-8">
      <div className="max-w-7xl mx-auto">
        <h1 className="text-4xl font-bold text-center mb-8 text-gray-800">
          {t.title}
        </h1>
        
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-8">
          {/* –õ–µ–≤–∞—è —á–∞—Å—Ç—å - –ê–≤–∞—Ç–∞—Ä */}
          <div className="bg-white rounded-2xl shadow-xl p-6">
            <div className="mb-4">
              <div className="flex items-center justify-between mb-4">
                <h2 className="text-xl font-semibold text-gray-700">{t.videoTitle}</h2>
                <div className={`px-3 py-1 rounded-full text-sm font-medium ${
                  isConnected ? 'bg-green-100 text-green-700' : 
                  isConnecting ? 'bg-yellow-100 text-yellow-700' : 
                  'bg-gray-100 text-gray-600'
                }`}>
                  {isConnected ? t.connected : isConnecting ? t.connecting : t.notConnected}
                </div>
              </div>
              
              <div className="relative bg-gray-900 rounded-xl overflow-hidden" style={{ aspectRatio: '16/9' }}>
                <video 
                  ref={videoRef} 
                  className="w-full h-full object-cover"
                  playsInline
                />
                <audio ref={audioRef} />
                
                {!isConnected && (
                  <div className="absolute inset-0 flex items-center justify-center bg-black bg-opacity-50">
                    <div className="text-center text-white">
                      {!isInitialized ? (
                        <div className="animate-pulse">{t.loadingSdk}</div>
                      ) : error ? (
                        <div className="text-red-400">{error}</div>
                      ) : (
                        <div className="text-gray-300">{t.notConnectedLabel}</div>
                      )}
                    </div>
                  </div>
                )}
              </div>
              
              <div className="mt-4 flex gap-3">
                {!isConnected ? (
                  <button
                    onClick={connectAvatar}
                    disabled={!isInitialized || isConnecting}
                    className="flex-1 bg-gradient-to-r from-purple-600 to-blue-600 text-white px-6 py-3 rounded-xl font-medium hover:from-purple-700 hover:to-blue-700 disabled:opacity-50 disabled:cursor-not-allowed transition-all"
                  >
                    {isConnecting ? t.connecting : t.startButton}
                  </button>
                ) : (
                  <button
                    onClick={() => {
                      if (avatarSynthesizerRef.current) {
                        avatarSynthesizerRef.current.close()
                      }
                      if (peerConnectionRef.current) {
                        peerConnectionRef.current.close()
                      }
                      setIsConnected(false)
                      setMessages([])
                    }}
                    className="flex-1 bg-red-500 text-white px-6 py-3 rounded-xl font-medium hover:bg-red-600 transition-all"
                  >
                    {t.stopButton}
                  </button>
                )}
              </div>
            </div>
          </div>
          
          {/* –ü—Ä–∞–≤–∞—è —á–∞—Å—Ç—å - –ß–∞—Ç */}
          <div className="bg-white rounded-2xl shadow-xl p-6 flex flex-col">
            <h2 className="text-xl font-semibold text-gray-700 mb-4">{t.dialogTitle}</h2>
            
            <div className="flex-1 overflow-y-auto mb-4 space-y-3 min-h-[400px] max-h-[500px]">
              {messages.map((msg, idx) => (
                <div
                  key={idx}
                  className={`p-3 rounded-lg ${
                    msg.role === 'user' 
                      ? 'bg-blue-500 text-white ml-auto max-w-[80%]' 
                      : msg.role === 'assistant'
                      ? 'bg-gray-100 text-gray-800 mr-auto max-w-[80%]'
                      : 'bg-yellow-50 text-yellow-800 text-center'
                  }`}
                >
                  {msg.text}
                </div>
              ))}
              {isSpeaking && (
                <div className="text-center text-gray-500 italic">
                  {t.speaking}
                </div>
              )}
              {isListening && (
                <div className="text-center text-purple-500 italic">
                  {listeningText}
                </div>
              )}
            </div>
            
            <div className="flex gap-3">
              <input
                type="text"
                value={userInput}
                onChange={(e) => setUserInput(e.target.value)}
                onKeyDown={(e) => e.key === 'Enter' && handleSendMessage()}
                placeholder={t.inputPlaceholder}
                disabled={!isConnected}
                className="flex-1 px-4 py-3 border-2 border-gray-200 rounded-xl focus:outline-none focus:border-purple-500 disabled:bg-gray-100 disabled:cursor-not-allowed"
              />
              <button
                onClick={startSpeechRecognition}
                disabled={!isConnected || isListening || isSpeaking}
                className="bg-white border-2 border-purple-200 text-purple-600 px-4 py-3 rounded-xl font-medium hover:border-purple-400 hover:text-purple-700 disabled:opacity-50 disabled:cursor-not-allowed transition-all"
              >
                {isListening ? t.listening : t.microphoneButton}
              </button>
              <button
                onClick={handleSendMessage}
                disabled={!isConnected || !userInput.trim()}
                className="bg-purple-600 text-white px-6 py-3 rounded-xl font-medium hover:bg-purple-700 disabled:opacity-50 disabled:cursor-not-allowed transition-all"
              >
                {t.sendButton}
              </button>
            </div>
          </div>
        </div>
      </div>
    </div>
  )
}
