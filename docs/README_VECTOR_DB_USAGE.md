# –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö

## üìö –û–±–∑–æ—Ä

–ù–∞ —Å–µ—Ä–≤–µ—Ä–µ —Å–æ–∑–¥–∞–Ω—ã –¥–≤–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **pgvecto.rs (vectors v0.4.0)**:

1. **vnd** - –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–í–ù–î)
2. **npa** - –ü—Ä–∞–≤–æ–≤—ã–µ –Ω–æ—Ä–º—ã –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω (–ù–ü–ê)

## üîå –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

```python
# –ë–∞–∑–∞ –í–ù–î
VND_DSN = "postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/vnd"

# –ë–∞–∑–∞ –ù–ü–ê
NPA_DSN = "postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/npa"
```

## üéØ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

### 1. Python –±–∏–±–ª–∏–æ—Ç–µ–∫–∏

```bash
pip install psycopg2-binary sentence-transformers torch numpy
```

### 2. –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

**–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!** –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ –Ω—É–∂–Ω–∞ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä—ã.

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –º–æ–¥–µ–ª—å:** `BAAI/bge-m3`
- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: 1024
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞: ‚úÖ
- –ö–∞—á–µ—Å—Ç–≤–æ: –≤—ã—Å–æ–∫–æ–µ
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 8192 —Ç–æ–∫–µ–Ω–æ–≤

```python
from sentence_transformers import SentenceTransformer

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ–¥–∏–Ω —Ä–∞–∑, –∑–∞—Ç–µ–º –∫–µ—à–∏—Ä—É–µ—Ç—Å—è)
model = SentenceTransformer("BAAI/bge-m3")
model.max_seq_length = 8192  # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏:**
- `intfloat/multilingual-e5-large` (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 1024)
- `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 768, –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤ –ë–î)

## üìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö "vnd" - –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü

```sql
-- –î–æ–∫—É–º–µ–Ω—Ç—ã
documents (
    id UUID PRIMARY KEY,
    filename VARCHAR(512),
    title VARCHAR(512),
    lang VARCHAR(10),
    sha256 VARCHAR(64),
    page_count INTEGER,
    created_at TIMESTAMP
)

-- –°–µ–∫—Ü–∏–∏ —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏
sections (
    id UUID PRIMARY KEY,
    document_id UUID,
    section_label VARCHAR(50),
    title TEXT,
    text TEXT,
    token_count INTEGER,
    char_count INTEGER,
    embedding vector(1024) NOT NULL,  -- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!
    created_at TIMESTAMP
)

-- –ü–æ–¥—Å–µ–∫—Ü–∏–∏ —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏
subsections (
    id UUID PRIMARY KEY,
    document_id UUID,
    section_id UUID,
    title TEXT,
    text TEXT,
    token_count INTEGER,
    char_count INTEGER,
    embedding vector(1024) NOT NULL,  -- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!
    created_at TIMESTAMP
)
```

### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –í–ù–î

```python
import psycopg2
from sentence_transformers import SentenceTransformer
import uuid

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
conn = psycopg2.connect("postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/vnd")
cur = conn.cursor()

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
model = SentenceTransformer("BAAI/bge-m3")
model.max_seq_length = 8192

# 1. –í—Å—Ç–∞–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
doc_id = str(uuid.uuid4())
cur.execute("""
    INSERT INTO documents (id, filename, title, lang)
    VALUES (%s, %s, %s, %s)
""", (doc_id, "example.pdf", "–ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞", "ru"))

# 2. –í—Å—Ç–∞–≤–∫–∞ —Å–µ–∫—Ü–∏–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º
section_text = "–°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç –æ–±—â–µ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –û–±—â–µ—Å—Ç–≤–∞..."

# –í–ê–ñ–ù–û: –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
embedding = model.encode([section_text], normalize_embeddings=True)[0]
embedding_list = embedding.tolist()  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫

cur.execute("""
    INSERT INTO sections (id, document_id, section_label, title, text, embedding)
    VALUES (%s, %s, %s, %s, %s, %s)
""", (
    str(uuid.uuid4()),
    doc_id,
    "1.1",
    "–°–æ–≤–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤",
    section_text,
    embedding_list  # –ü–µ—Ä–µ–¥–∞–µ–º –∫–∞–∫ —Å–ø–∏—Å–æ–∫
))

conn.commit()
cur.close()
conn.close()
```

### –ü–æ–∏—Å–∫ –≤ –í–ù–î

```python
import psycopg2
from sentence_transformers import SentenceTransformer

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
conn = psycopg2.connect("postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/vnd")
cur = conn.cursor()

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = SentenceTransformer("BAAI/bge-m3")
model.max_seq_length = 8192

# –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
query = "–∫–∞–∫–∏–µ –ø–æ–ª–Ω–æ–º–æ—á–∏—è —É —Å–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤?"

# –í–ê–ñ–ù–û: –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
query_embedding = model.encode([query], normalize_embeddings=True)[0].tolist()

# –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–µ–∫—Ü–∏—è–º
cur.execute("""
    SELECT 
        s.title,
        s.text,
        d.filename,
        1 - (s.embedding <=> %s::vector) AS similarity
    FROM sections s
    JOIN documents d ON s.document_id = d.id
    ORDER BY s.embedding <=> %s::vector
    LIMIT 10
""", (query_embedding, query_embedding))

results = cur.fetchall()
for title, text, filename, similarity in results:
    print(f"–°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.3f}")
    print(f"–î–æ–∫—É–º–µ–Ω—Ç: {filename}")
    print(f"–†–∞–∑–¥–µ–ª: {title}")
    print(f"–¢–µ–∫—Å—Ç: {text[:200]}...")
    print("-" * 80)

cur.close()
conn.close()
```

### –û–ø–µ—Ä–∞—Ç–æ—Ä `<=>` - –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ

- `<=>` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç 0 (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ) –¥–æ 2 (–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ)
- `1 - (embedding <=> query)` = –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –æ—Ç -1 –¥–æ 1
- –ß–µ–º –º–µ–Ω—å—à–µ `<=>`, —Ç–µ–º –±–ª–∏–∂–µ –≤–µ–∫—Ç–æ—Ä—ã

## üìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö "npa" - –ü—Ä–∞–≤–æ–≤—ã–µ –Ω–æ—Ä–º—ã

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü

```sql
-- –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
document_metadata (
    id VARCHAR(255) PRIMARY KEY,
    title TEXT NOT NULL,
    doc_type VARCHAR(100),
    doc_number VARCHAR(100),
    adoption_date DATE,
    status VARCHAR(50),
    source_url TEXT,
    created_at TIMESTAMP
)

-- –ß–∞–Ω–∫–∏ —Å dense –∏ sparse –≤–µ–∫—Ç–æ—Ä–∞–º–∏
content_chunks (
    id SERIAL PRIMARY KEY,
    doc_id VARCHAR(255),
    metadata TEXT,
    chunk TEXT NOT NULL,
    chunk_tsv tsvector,  -- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ —Ç—Ä–∏–≥–≥–µ—Ä
    embedding vector(1024) NOT NULL,  -- Dense –≤–µ–∫—Ç–æ—Ä - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!
    sparse_embedding svector(250002) NOT NULL,  -- Sparse –≤–µ–∫—Ç–æ—Ä - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!
    created_at TIMESTAMP
)
```

### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ù–ü–ê

–î–ª—è –ù–ü–ê –Ω—É–∂–Ω—ã **–î–í–ê —Ç–∏–ø–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:**
1. **Dense** - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã (1024)
2. **Sparse** - –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã (250002)

```python
import psycopg2
from FlagEmbedding import BGEM3FlagModel
import torch

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
conn = psycopg2.connect("postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/npa")
cur = conn.cursor()

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è dense –∏ sparse –≤–µ–∫—Ç–æ—Ä–æ–≤
device = torch.device("mps" if torch.backends.mps.is_available() else 
                     ("cuda" if torch.cuda.is_available() else "cpu"))
model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=torch.cuda.is_available(), device=device)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
def create_embeddings(text: str):
    """–°–æ–∑–¥–∞–µ—Ç dense –∏ sparse –≤–µ–∫—Ç–æ—Ä—ã"""
    output = model.encode(
        [text], 
        return_dense=True, 
        return_sparse=True, 
        return_colbert_vecs=False
    )
    
    # Dense –≤–µ–∫—Ç–æ—Ä
    dense_vec = output['dense_vecs'][0].tolist()
    
    # Sparse –≤–µ–∫—Ç–æ—Ä
    sparse_dict = output['lexical_weights'][0]
    sparse_indices = sorted([int(k) for k in sparse_dict.keys()])
    sparse_values = [float(sparse_dict[str(idx)]) for idx in sparse_indices]
    
    return dense_vec, sparse_indices, sparse_values

# 1. –í—Å—Ç–∞–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞
doc_id = "123456"
cur.execute("""
    INSERT INTO document_metadata (id, title, doc_type)
    VALUES (%s, %s, %s)
""", (doc_id, "–ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å –†–ö", "–ö–æ–¥–µ–∫—Å"))

# 2. –í—Å—Ç–∞–≤–∫–∞ —á–∞–Ω–∫–∞ —Å –æ–±–æ–∏–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏
chunk_text = "–°—Ç–∞—Ç—å—è 1. –ù–∞–ª–æ–≥–æ–≤–æ–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω..."

# –í–ê–ñ–ù–û: –°–æ–∑–¥–∞–µ–º –æ–±–∞ —Ç–∏–ø–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
dense_vec, sparse_indices, sparse_values = create_embeddings(chunk_text)

# –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º sparse –≤–µ–∫—Ç–æ—Ä –¥–ª—è PostgreSQL
sparse_str = f"{{1:{{{','.join(map(str, sparse_indices))}}}:{{{','.join(map(str, sparse_values))}}}}}"

cur.execute("""
    INSERT INTO content_chunks (doc_id, metadata, chunk, embedding, sparse_embedding)
    VALUES (%s, %s, %s, %s, %s::svector)
""", (
    doc_id,
    "–°—Ç–∞—Ç—å—è 1",
    chunk_text,
    dense_vec,
    sparse_str
))

conn.commit()
cur.close()
conn.close()
```

### –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –ù–ü–ê

–ù–ü–ê –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **3 —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞:**
1. **BM25** - –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
2. **Dense** - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
3. **Sparse** - –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫

```python
import psycopg2
from FlagEmbedding import BGEM3FlagModel
import torch

conn = psycopg2.connect("postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/npa")
cur = conn.cursor()

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
device = torch.device("mps" if torch.backends.mps.is_available() else 
                     ("cuda" if torch.cuda.is_available() else "cpu"))
model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=torch.cuda.is_available(), device=device)

query = "–∫–∞–∫–∏–µ –Ω–∞–ª–æ–≥–∏ –ø–ª–∞—Ç–∏—Ç –¢–û–û?"

# –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–ø—Ä–æ—Å–∞
output = model.encode([query], return_dense=True, return_sparse=True, return_colbert_vecs=False)
dense_vec = output['dense_vecs'][0].tolist()
sparse_dict = output['lexical_weights'][0]
sparse_indices = sorted([int(k) for k in sparse_dict.keys()])
sparse_values = [float(sparse_dict[str(idx)]) for idx in sparse_indices]

# 1. BM25 –ø–æ–∏—Å–∫ (–ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π)
cur.execute("""
    SELECT doc_id, chunk, 
           ts_rank_cd(chunk_tsv, plainto_tsquery('russian', %s)) AS score
    FROM content_chunks
    WHERE chunk_tsv @@ plainto_tsquery('russian', %s)
    ORDER BY score DESC
    LIMIT 10
""", (query, query))
bm25_results = cur.fetchall()

# 2. Dense –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π)
cur.execute("""
    SELECT doc_id, chunk,
           1 - (embedding <=> %s::vector) AS score
    FROM content_chunks
    ORDER BY embedding <=> %s::vector
    LIMIT 10
""", (dense_vec, dense_vec))
dense_results = cur.fetchall()

# 3. Sparse –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (–ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π)
sparse_str = f"{{1:{{{','.join(map(str, sparse_indices))}}}:{{{','.join(map(str, sparse_values))}}}}}"
cur.execute("""
    SELECT doc_id, chunk,
           1 - (sparse_embedding <=> %s::svector) AS score
    FROM content_chunks
    ORDER BY sparse_embedding <=> %s::svector
    LIMIT 10
""", (sparse_str, sparse_str))
sparse_results = cur.fetchall()

# 4. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å RRF (Reciprocal Rank Fusion)
def hybrid_rrf(results_list, k=60):
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""
    rrf_scores = {}
    
    for results in results_list:
        for rank, (doc_id, chunk, score) in enumerate(results):
            key = (doc_id, chunk)
            if key not in rrf_scores:
                rrf_scores[key] = 0
            rrf_scores[key] += 1 / (k + rank)
    
    return sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

hybrid_results = hybrid_rrf([bm25_results, dense_results, sparse_results])

for (doc_id, chunk), score in hybrid_results[:10]:
    print(f"Score: {score:.3f}")
    print(f"Doc ID: {doc_id}")
    print(f"–¢–µ–∫—Å—Ç: {chunk[:200]}...")
    print("-" * 80)

cur.close()
conn.close()
```

## üîß –ì–æ—Ç–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –î–ª—è –í–ù–î (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ existing –∫–æ–¥)

–£–∂–µ –µ—Å—Ç—å –≥–æ—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤ `–í–ù–î/internal_search.py`:

```python
from –í–ù–î.internal_search import internal_doc_search

# –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
results = internal_doc_search("–ø–æ–ª–Ω–æ–º–æ—á–∏—è —Å–æ–≤–µ—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤")

for result in results['results']:
    print(f"–î–æ–∫—É–º–µ–Ω—Ç: {result['doc_title']}")
    print(f"–†–∞–∑–¥–µ–ª: {result['title']}")
    print(f"–°—Ö–æ–¥—Å—Ç–≤–æ: {result['similarity']:.3f}")
    print(f"–¢–µ–∫—Å—Ç: {result['text'][:200]}...")
    print("-" * 80)
```

**–í–ê–ñ–ù–û:** –û–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–æ–∫—É –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≤ `internal_search.py`:

```python
# –í —Ñ–∞–π–ª–µ –í–ù–î/internal_search.py
class InternalSearchConfig(BaseModel):
    # ...
    DB_DSN: str = Field(default="postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/vnd")
```

### –î–ª—è –ù–ü–ê (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ existing –∫–æ–¥)

–£–∂–µ –µ—Å—Ç—å –≥–æ—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤ `–ü—Ä–∞–≤–æ–≤—ã–µ –Ω–æ—Ä–º—ã/rag_npa.py`:

```python
from –ü—Ä–∞–≤–æ–≤—ã–µ –Ω–æ—Ä–º—ã.rag_npa import legal_search, legal_generate_answer

# –ü–æ–∏—Å–∫
results = legal_search("–Ω–∞–ª–æ–≥–∏ –¥–ª—è –¢–û–û", top_k=5)

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
for r in results['results']:
    print(f"–î–æ–∫—É–º–µ–Ω—Ç: {r['title']}")
    print(f"–°—Ö–æ–¥—Å—Ç–≤–æ: {r['score']:.3f}")
    print(f"–¢–µ–∫—Å—Ç: {r['chunk'][:200]}...")
    print(f"–°—Å—ã–ª–∫–∞: {r['link']}")
    print("-" * 80)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
answer = legal_generate_answer(results['query'], results['results'])
print(answer)
```

**–í–ê–ñ–ù–û:** –û–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–æ–∫—É –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≤ `rag_npa.py`:

```python
# –í —Ñ–∞–π–ª–µ –ü—Ä–∞–≤–æ–≤—ã–µ –Ω–æ—Ä–º—ã/rag_npa.py
def get_npa_conn():
    return psycopg2.connect(
        "postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/npa"
    )
```

## ‚öôÔ∏è –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

–°–æ–∑–¥–∞–π—Ç–µ `.env` —Ñ–∞–π–ª:

```bash
# –ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
VND_DB_DSN=postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/vnd
NPA_DB_DSN=postgresql://<YOUR_DB_USER>:<YOUR_DB_PASSWORD>@<YOUR_DB_HOST>:<YOUR_DB_PORT>/npa

# –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
EMBED_MODEL=BAAI/bge-m3

# OpenAI (–¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤)
OPENAI_API_KEY=your_key_here
```

## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –ò–Ω–¥–µ–∫—Å—ã HNSW

–í –±–∞–∑–∞—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º HNSW —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:
- `m = 16` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π –Ω–∞ —É–∑–µ–ª
- `ef_construction = 64` - —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è

–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ (–º–µ–¥–ª–µ–Ω–Ω–µ–µ):
```sql
-- –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å —Å –±–æ–ª—å—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
DROP INDEX sections_embedding_idx;
CREATE INDEX sections_embedding_idx 
ON sections USING vectors (embedding vector_cos_ops)
WITH (options = $$
    [indexing.hnsw]
    m = 32
    ef_construction = 128
$$);
```

### Vacuum –∏ analyze

–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–π—Ç–µ:
```sql
VACUUM ANALYZE sections;
VACUUM ANALYZE subsections;
VACUUM ANALYZE content_chunks;
```

## üö® –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

### 1. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´

‚ùå **–ù–µ–ª—å–∑—è –≤—Å—Ç–∞–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:**
```python
# –≠–¢–û –ù–ï –†–ê–ë–û–¢–ê–ï–¢!
cur.execute("""
    INSERT INTO sections (document_id, title, text)
    VALUES (%s, %s, %s)
""", (doc_id, "–ó–∞–≥–æ–ª–æ–≤–æ–∫", "–¢–µ–∫—Å—Ç"))
# ERROR: null value in column "embedding"
```

‚úÖ **–í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏:**
```python
# –ü–†–ê–í–ò–õ–¨–ù–û!
embedding = model.encode([text], normalize_embeddings=True)[0].tolist()
cur.execute("""
    INSERT INTO sections (document_id, title, text, embedding)
    VALUES (%s, %s, %s, %s)
""", (doc_id, "–ó–∞–≥–æ–ª–æ–≤–æ–∫", "–¢–µ–∫—Å—Ç", embedding))
```

### 2. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤

- –í–ù–î: `vector(1024)` - —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
- –ù–ü–ê: `vector(1024)` –¥–ª—è dense, `svector(250002)` –¥–ª—è sparse

–ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å —Å –¥—Ä—É–≥–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é, –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —Å—Ö–µ–º—É –ë–î.

### 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

–î–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ **—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å** –≤–µ–∫—Ç–æ—Ä—ã:

```python
embedding = model.encode([text], normalize_embeddings=True)[0]
```

### 4. –û–ø–µ—Ä–∞—Ç–æ—Ä `<=>` vs `<->`

- `<=>` - –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤)
- `<->` - –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
- `<#>` - –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ

–í –Ω–∞—à–∏—Ö –±–∞–∑–∞—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `<=>` (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ).

## üîç –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤

### –ü–æ–∏—Å–∫ –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π

```sql
-- –ü–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
SELECT title, text, 1 - (embedding <=> %s::vector) AS similarity
FROM sections
WHERE document_id = %s
  AND 1 - (embedding <=> %s::vector) > 0.5
ORDER BY similarity DESC
LIMIT 10;
```

### –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º

```sql
-- –õ—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
SELECT DISTINCT ON (document_id)
    document_id,
    title,
    text,
    1 - (embedding <=> %s::vector) AS similarity
FROM sections
ORDER BY document_id, similarity DESC;
```

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–µ–∫—Ç–æ—Ä–∞–º

```sql
-- –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤
SELECT 
    COUNT(*) as total,
    COUNT(embedding) as with_embedding,
    COUNT(*) - COUNT(embedding) as without_embedding
FROM sections;
```

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [pgvecto.rs –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https://docs.pgvecto.rs/)
- [BAAI/bge-m3 –Ω–∞ HuggingFace](https://huggingface.co/BAAI/bge-m3)
- [Sentence Transformers](https://www.sbert.net/)

## üÜò –ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

### –ü—Ä–æ–±–ª–µ–º–∞: "type vector does not exist"

**–†–µ—à–µ–Ω–∏–µ:** –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –±–∞–∑–µ
```sql
CREATE EXTENSION IF NOT EXISTS vectors;
```

### –ü—Ä–æ–±–ª–µ–º–∞: "operator class vector_cosine_ops does not exist"

**–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä –¥–ª—è pgvecto.rs
```sql
-- –ù–ï vector_cosine_ops
-- –ê vector_cos_ops
CREATE INDEX ... USING vectors (embedding vector_cos_ops);
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–µ–¥–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫

**–†–µ—à–µ–Ω–∏—è:**
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤: `\di`
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ `VACUUM ANALYZE`
3. –£–≤–µ–ª–∏—á—å—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã HNSW –∏–Ω–¥–µ–∫—Å–∞

### –ü—Ä–æ–±–ª–µ–º–∞: "Dimensions type modifier needed"

**–†–µ—à–µ–Ω–∏–µ:** –£–∫–∞–∂–∏—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è sparse –≤–µ–∫—Ç–æ—Ä–æ–≤
```sql
-- –ù–ï svector
-- –ê svector(250002)
sparse_embedding svector(250002) NOT NULL
```

## üéì –ì–æ—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã

–ü–æ–ª–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Å–º. –≤ —Ñ–∞–π–ª–∞—Ö:
- `–í–ù–î/internal_search.py` - –ø–æ–∏—Å–∫ –ø–æ –í–ù–î
- `–ü—Ä–∞–≤–æ–≤—ã–µ –Ω–æ—Ä–º—ã/rag_npa.py` - –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –ù–ü–ê

---

**–°–æ–∑–¥–∞–Ω–æ:** 5 —è–Ω–≤–∞—Ä—è 2025  
**–í–µ—Ä—Å–∏—è pgvecto.rs:** 0.4.0  
**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –º–æ–¥–µ–ª—å:** BAAI/bge-m3
